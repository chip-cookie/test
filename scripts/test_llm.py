#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Multi-LLM í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (Windows + PyCharm í˜¸í™˜)

OpenAI, Groq, Gemini ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
"""

import asyncio
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.llm.multi_llm import MultiLLMOrchestrator
from src.llm.providers import ProviderConfig
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


async def main():
    """Multi-LLM í…ŒìŠ¤íŠ¸"""

    print("=" * 60)
    print("Multi-LLM ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # API í‚¤ í™•ì¸
    openai_key = os.getenv("OPENAI_API_KEY")
    groq_key = os.getenv("GROQ_API_KEY")
    gemini_key = os.getenv("GEMINI_API_KEY")

    if not openai_key:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— API í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return

    print(f"\nâœ… OpenAI API í‚¤: {openai_key[:20]}...")
    print(f"âœ… Groq API í‚¤: {groq_key[:20] if groq_key else 'ë¯¸ì„¤ì •'}...")
    print(f"âœ… Gemini API í‚¤: {gemini_key[:20] if gemini_key else 'ë¯¸ì„¤ì •'}...")

    # Multi-LLM ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„±
    orchestrator = MultiLLMOrchestrator()

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    query = "ì²­ë…„ ì£¼ê±° ì§€ì› ì •ì±…ì—ëŠ” ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”?"
    context = """
    ì²­ë…„ ì£¼ê±° ì§€ì› ì •ì±…:
    1. ì²­ë…„ ì›”ì„¸ ì§€ì›: ë§Œ 19~34ì„¸, ì›” ìµœëŒ€ 20ë§Œì›
    2. ì²­ë…„ ì „ì„¸ëŒ€ì¶œ: ì—° 1~2% ì €ê¸ˆë¦¬, ìµœëŒ€ 2ì–µì›
    3. ì£¼ê±°ê¸‰ì—¬: ì¤‘ìœ„ì†Œë“ 50% ì´í•˜, ì›”ì„¸ ì§€ì›
    """

    print(f"\nì§ˆë¬¸: {query}")
    print(f"ì»¨í…ìŠ¤íŠ¸: {context[:100]}...")

    # Multi-LLM í˜¸ì¶œ
    print("\nğŸš€ Multi-LLM ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
    result = await orchestrator.generate_with_best(
        prompt=query,
        context=context
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ì‘ë‹µ ê²°ê³¼")
    print("=" * 60)

    print(f"\nğŸ† ìµœê³  í’ˆì§ˆ: {result.best_response.provider}")
    print(f"   ì ìˆ˜: {result.evaluation.total_score:.2f}/100")
    print(f"   ì‘ë‹µ ì‹œê°„: {result.best_response.latency:.2f}ì´ˆ")
    print(f"\n   ì‘ë‹µ ë‚´ìš©:\n{result.best_response.content[:200]}...\n")

    # ëª¨ë“  ì‘ë‹µ ë¹„êµ
    print("\nğŸ“ˆ ì „ì²´ ì‘ë‹µ ë¹„êµ:")
    for eval_result in result.all_evaluations:
        provider = eval_result.provider
        score = eval_result.total_score
        response = next(r for r in result.all_responses if r.provider == provider)
        latency = response.latency

        print(f"\n   [{provider.upper()}]")
        print(f"   - ì ìˆ˜: {score:.2f}/100")
        print(f"   - ì†ë„: {latency:.2f}ì´ˆ")
        print(f"   - ê°•ì : {', '.join(eval_result.strengths[:2])}")


if __name__ == "__main__":
    # Windowsì—ì„œ asyncio ì´ë²¤íŠ¸ ë£¨í”„ ì •ì±… ì„¤ì •
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # LLM í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(main())
