#!/usr/bin/env python3
"""
Vector Databaseì— ì²­ë…„ ì •ì±… ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚½ì…í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/insert_sample_data.py --tier1 vector-db/sample-data/tier1-samples.json
    python scripts/insert_sample_data.py --all  # Tier 1ê³¼ Tier 2 ëª¨ë‘ ì‚½ì…
"""

import json
import os
import sys
import argparse
import time
from typing import List, Dict, Any
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Pinecone ì„¤ì •
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX = os.getenv("PINECONE_INDEX", "youth-policy-kb")

# OpenAI ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-ada-002")

# ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ
DEFAULT_TIER1_PATH = "vector-db/sample-data/tier1-samples.json"
DEFAULT_TIER2_PATH = "vector-db/sample-data/tier2-samples.json"


def check_dependencies():
    """í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    missing = []

    try:
        from pinecone import Pinecone
    except ImportError:
        missing.append("pinecone-client")

    try:
        from openai import OpenAI
    except ImportError:
        missing.append("openai")

    if missing:
        print(f"âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing)}")
        print(f"   ì„¤ì¹˜: pip install {' '.join(missing)}")
        sys.exit(1)


def check_env_variables():
    """í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    missing = []

    if not PINECONE_API_KEY:
        missing.append("PINECONE_API_KEY")
    if not OPENAI_API_KEY:
        missing.append("OPENAI_API_KEY")

    if missing:
        print(f"âŒ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing)}")
        print("   .env íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)


def create_embedding(client, text: str) -> List[float]:
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    response = client.embeddings.create(
        input=text,
        model=EMBEDDING_MODEL
    )
    return response.data[0].embedding


def load_documents(file_path: str) -> List[Dict[str, Any]]:
    """JSON íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        sys.exit(1)

    with open(file_path, 'r', encoding='utf-8') as f:
        documents = json.load(f)

    print(f"ğŸ“„ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {file_path}")
    return documents


def insert_documents(index, client, documents: List[Dict[str, Any]], batch_size: int = 10):
    """ë¬¸ì„œë¥¼ Vector DBì— ì‚½ì…"""
    vectors = []
    total = len(documents)

    for i, doc in enumerate(documents, 1):
        print(f"   [{i}/{total}] ì„ë² ë”© ìƒì„± ì¤‘: {doc['metadata']['policy_name'][:30]}...")

        # ì„ë² ë”© ìƒì„±
        embedding = create_embedding(client, doc['content'])

        vectors.append({
            'id': doc['id'],
            'values': embedding,
            'metadata': doc['metadata']
        })

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—…ì„œíŠ¸
        if len(vectors) >= batch_size:
            index.upsert(vectors=vectors)
            vectors = []
            time.sleep(0.5)  # Rate limit ë°©ì§€

    # ë‚¨ì€ ë²¡í„° ì—…ì„œíŠ¸
    if vectors:
        index.upsert(vectors=vectors)

    print(f"âœ… {total}ê°œì˜ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤.")


def get_index_stats(index):
    """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ì¶œë ¥"""
    stats = index.describe_index_stats()
    print(f"\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
    print(f"   - ì´ ë²¡í„° ìˆ˜: {stats.total_vector_count}")
    print(f"   - ì°¨ì›: {stats.dimension}")


def main():
    parser = argparse.ArgumentParser(
        description='Vector DBì— ì²­ë…„ ì •ì±… ìƒ˜í”Œ ë°ì´í„° ì‚½ì…'
    )
    parser.add_argument(
        '--tier1',
        type=str,
        help=f'Tier 1 ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: {DEFAULT_TIER1_PATH})'
    )
    parser.add_argument(
        '--tier2',
        type=str,
        help=f'Tier 2 ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: {DEFAULT_TIER2_PATH})'
    )
    parser.add_argument(
        '--all',
        action='store_true',
        help='Tier 1ê³¼ Tier 2 ëª¨ë‘ ì‚½ì…'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)'
    )

    args = parser.parse_args()

    # í™˜ê²½ í™•ì¸
    print("ğŸ” í™˜ê²½ í™•ì¸ ì¤‘...")
    check_dependencies()
    check_env_variables()

    # ë¼ì´ë¸ŒëŸ¬ë¦¬ import (í™˜ê²½ í™•ì¸ í›„)
    from pinecone import Pinecone
    from openai import OpenAI

    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    print("ğŸ”Œ ì„œë¹„ìŠ¤ ì—°ê²° ì¤‘...")
    pc = Pinecone(api_key=PINECONE_API_KEY)
    index = pc.Index(PINECONE_INDEX)
    openai_client = OpenAI(api_key=OPENAI_API_KEY)

    print(f"   - Pinecone ì¸ë±ìŠ¤: {PINECONE_INDEX}")
    print(f"   - ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}")

    # ì‚½ì…í•  íŒŒì¼ ê²°ì •
    files_to_insert = []

    if args.all:
        files_to_insert = [DEFAULT_TIER1_PATH, DEFAULT_TIER2_PATH]
    else:
        if args.tier1:
            files_to_insert.append(args.tier1)
        if args.tier2:
            files_to_insert.append(args.tier2)

        if not files_to_insert:
            # ì¸ìê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ Tier 1ë§Œ ì‚½ì…
            files_to_insert = [DEFAULT_TIER1_PATH]

    # ë°ì´í„° ì‚½ì…
    total_inserted = 0
    for file_path in files_to_insert:
        print(f"\nğŸ“¥ ë°ì´í„° ì‚½ì… ì‹œì‘: {file_path}")
        documents = load_documents(file_path)
        insert_documents(index, openai_client, documents, args.batch_size)
        total_inserted += len(documents)

    # ìµœì¢… í†µê³„
    get_index_stats(index)

    print(f"\nğŸ‰ ì™„ë£Œ! ì´ {total_inserted}ê°œì˜ ë¬¸ì„œê°€ ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
